{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHf4J8utBdl5tuY9CyqPhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maryamshahzadi-123/Quarter-2/blob/main/Project_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install langchain google-cloud-aiplatform openai\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "PROJECT_ID = \"your-project-id\"\n",
        "LOCATION = \"us-central1\"\n",
        "MODEL_ID = \"projects/your-project-id/locations/us-central1/models/gemini-flash-2\"\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "template = \"\"\"You are a helpful assistant specializing in answering questions.\n",
        "The user asks: \"{question}\"\n",
        "Provide a concise and informative response.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "class GeminiLLM(OpenAI):\n",
        "    def __init__(self, project_id, model_id, temperature=0.7):\n",
        "        self.project_id = project_id\n",
        "        self.model_id = model_id\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def call(self, prompt):\n",
        "        client = aiplatform.gapic.PredictionServiceClient()\n",
        "        endpoint = f\"projects/{self.project_id}/locations/{LOCATION}/models/{self.model_id}\"\n",
        "        response = client.predict(\n",
        "            endpoint=endpoint,\n",
        "            instances=[{\"content\": prompt}],\n",
        "            parameters={\"temperature\": self.temperature}\n",
        "        )\n",
        "        return response.predictions[0]['content']\n",
        "\n",
        "gemini_llm = GeminiLLM(project_id=PROJECT_ID, model_id=MODEL_ID)\n",
        "\n",
        "llm_chain = LLMChain(llm=gemini_llm, prompt=prompt)\n",
        "\n",
        "questions = [\n",
        "    \"What are the benefits of AI in healthcare?\",\n",
        "    \"How can businesses use social media for marketing?\",\n",
        "    \"What is the importance of renewable energy?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Response: {llm_chain.run(question)}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "template_experiment = \"\"\"You are an expert assistant. Provide detailed insights into the following query: \"{question}\".\"\"\"\n",
        "prompt_experiment = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=template_experiment\n",
        ")\n",
        "\n",
        "llm_chain_experiment = LLMChain(llm=gemini_llm, prompt=prompt_experiment)\n",
        "\n",
        "print(\"\\nExperimenting with a new prompt and parameters...\")\n",
        "for question in questions:\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Response: {llm_chain_experiment.run(question)}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "sEYHDORd9jgf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}